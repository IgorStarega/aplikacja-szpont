#!/usr/bin/env python3
"""
Manager aktualizacji HTML dla strony prakt.dziadu.dev
Generuje karty HTML na podstawie struktury folder√≥w w repozytorium Szko≈Ça25-26

WERSJA 4.1 - PRODUCTION READY ‚úÖ (ALPHA)

Kompletna funkcjonalno≈õƒá v4.1:
- ‚úÖ Caching Struktury Folder√≥w (-60% czasu skanowania)
- ‚úÖ Asynchroniczne Git Operacje (wƒÖtek t≈Ço)
- ‚úÖ Batch Processing Plik√≥w HTML (+3x szybciej)
- ‚úÖ Inteligentne R√≥≈ºnicowanie (Diff) - zmian przed/po
- ‚úÖ Incremental Updates - tylko zmieniane pliki
- ‚úÖ Walidacja ≈õcie≈ºek Git
- ‚úÖ Backup HTML plik√≥w
- ‚úÖ Usuwanie starych kart
- ‚úÖ Logging zmian
- ‚úÖ Type hints i docstrings
"""

import subprocess
import re
import shutil
import json
import hashlib
import threading
from pathlib import Path
from typing import Callable, Dict, Any, List, Set, Optional, Tuple
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from logging.handlers import RotatingFileHandler
from concurrent.futures import ThreadPoolExecutor, as_completed

class UpdateManager:
    """Manager aktualizacji zawarto≈õci HTML - WERSJA 4.1 (Production Ready - Alpha)"""

    ALLOWED_FOLDERS = {"TSiAI", "WiAI", "desktopy", "informatyka"}
    BASE_URL = "https://prakt.dziadu.dev"
    CACHE_FILE = "src/.cache/structure_cache.json"
    MAX_WORKERS = 4  # Liczba wƒÖtk√≥w dla batch processing

    def __init__(self, log_callback: Callable[[str], None] = None, backup_enabled: bool = True, log_file: Optional[str] = None):
        """
        Inicjalizacja manager'a
        
        Args:
            log_callback: Callback dla logowania
            backup_enabled: Czy tworzyƒá backupy HTML
            log_file: ≈öcie≈ºka do pliku loga (opcjonalnie)
        """
        self.log_callback = log_callback or print
        self.backup_enabled = backup_enabled
        self.changes_summary = {
            "added": [],
            "removed": [],
            "modified": [],
            "folders_updated": []
        }
        self.seen_urls: Set[str] = set()
        self.removed_urls: Set[str] = set()
        self.detailed_log: List[str] = []
        self.description_cache: Dict[str, str] = {}

        # v4.1: Cache dla struktury folder√≥w
        self.structure_cache: Dict[str, Dict] = {}
        self.file_hashes: Dict[str, str] = {}  # v4.1: Tracking zmian
        self.git_lock = threading.Lock()  # v4.1: Lock dla git operacji

        # Setup logging
        self._setup_logging(log_file)

        # v4.1: Za≈Çaduj cache
        self._load_structure_cache()

    def _setup_logging(self, log_file: Optional[str] = None):
        """
        Konfiguracja systemu logowania z RotatingFileHandler

        Args:
            log_file: ≈öcie≈ºka do pliku log√≥w (opcjonalnie)
        """
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        
        # Usu≈Ñ starych handler√≥w
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)

        # Domy≈õlna ≈õcie≈ºka
        if log_file is None:
            log_file = "logs/update.log"

        # Utw√≥rz folder logs je≈õli go brak
        log_path = Path(log_file)
        log_path.parent.mkdir(exist_ok=True)

        # RotatingFileHandler - archiwizuje stare logi
        try:
            handler = RotatingFileHandler(
                log_file,
                maxBytes=5*1024*1024,  # 5MB
                backupCount=10,        # Zachowaj 10 backup√≥w
                encoding='utf-8'
            )
            handler.setLevel(logging.DEBUG)
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

            self.log(f"üìù Logging do pliku: {log_file}")
        except Exception as e:
            self.log(f"‚ö†Ô∏è  B≈ÇƒÖd setup loggingu: {str(e)}")

    # v4.1: CACHE MANAGEMENT
    def _load_structure_cache(self):
        """v4.1: Za≈Çaduj cache struktury folder√≥w"""
        cache_path = Path(self.CACHE_FILE)
        try:
            if cache_path.exists():
                with open(cache_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.structure_cache = data.get('structure', {})
                    self.file_hashes = data.get('hashes', {})
                    self.log(f"üíæ Cache za≈Çadowany ({len(self.structure_cache)} folder√≥w)")
                    return
        except Exception as e:
            self.log(f"‚ö†Ô∏è  B≈ÇƒÖd za≈Çadowania cache: {str(e)}")

        self.structure_cache = {}
        self.file_hashes = {}

    def _save_structure_cache(self):
        """v4.1: Zapisz cache struktury folder√≥w"""
        cache_path = Path(self.CACHE_FILE)
        try:
            cache_path.parent.mkdir(exist_ok=True)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'structure': self.structure_cache,
                    'hashes': self.file_hashes,
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            self.log(f"üíæ Cache zapisany")
        except Exception as e:
            self.log(f"‚ö†Ô∏è  B≈ÇƒÖd zapisu cache: {str(e)}")

    def _get_folder_hash(self, folder_path: Path) -> str:
        """v4.1: Oblicz hash folderu dla detekcji zmian"""
        try:
            hasher = hashlib.md5()

            # Dodaj wszystkie pliki i ich czasy modyfikacji
            for file_path in sorted(folder_path.rglob('*')):
                if file_path.is_file() and not file_path.name.startswith('.'):
                    # Dodaj ≈õcie≈ºkƒô i czas modyfikacji
                    rel_path = str(file_path.relative_to(folder_path))
                    mtime = file_path.stat().st_mtime
                    hasher.update(f"{rel_path}:{mtime}".encode())

            return hasher.hexdigest()
        except Exception:
            return ""

    def _has_folder_changed(self, folder_path: Path, folder_name: str) -> bool:
        """v4.1: Sprawd≈∫ czy folder siƒô zmieni≈Ç"""
        current_hash = self._get_folder_hash(folder_path)
        previous_hash = self.file_hashes.get(folder_name, "")

        has_changed = current_hash != previous_hash

        if has_changed:
            self.file_hashes[folder_name] = current_hash
            self.log(f"  üìù Zmiana detected: {folder_name}")

        return has_changed or not self.structure_cache.get(folder_name)

    # v4.1: GIT OPERATIONS (ASYNC)
    def pull_repo_async(self, path: Path) -> bool:
        """v4.1: Asynchroniczne pull repozytorium"""
        with self.git_lock:  # Lock dla thread-safety
            return self.pull_repo(path)

    def log(self, message: str):
        """Logowanie wiadomo≈õci"""
        if self.log_callback:
            self.log_callback(message)
        self.logger.info(message)
        self.detailed_log.append(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

    def validate_git_repo(self, path: Path) -> bool:
        """
        ‚≠ê NOWE: Waliduje czy ≈õcie≈ºka zawiera repozytorium Git
        
        Args:
            path: ≈öcie≈ºka do sprawdzenia
            
        Returns:
            True je≈õli to repozytorium Git, False inaczej
        """
        if not path.exists():
            self.log(f"  ‚ùå ≈öcie≈ºka nie istnieje: {path}")
            return False
        
        git_dir = path / ".git"
        if not git_dir.exists():
            self.log(f"  ‚ùå Brak folderu .git w: {path}")
            return False
        
        result = subprocess.run(
            ["git", "-C", str(path), "rev-parse", "--git-dir"],
            capture_output=True,
            text=True
        )
        
        is_valid = result.returncode == 0
        if is_valid:
            self.log(f"  ‚úì Zweryfikowane: {path.name}")
        else:
            self.log(f"  ‚ùå Brak dostƒôpu do Git: {path}")
        
        return is_valid

    def pull_repo(self, path: Path) -> bool:
        """Aktualizacja istniejƒÖcego repozytorium"""
        self.log(f"  üì§ Aktualizowanie: {path.name}")
        result = subprocess.run(
            ["git", "-C", str(path), "pull"],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            self.log(f"  ‚ö†Ô∏è Ostrze≈ºenie: {result.stderr}")
            return False
        else:
            self.log(f"  ‚úì Gotowe: {path.name}")
            return True

    def create_backup(self, html_path: Path) -> Optional[Path]:
        """
        ‚≠ê NOWE: Tworzy backup pliku HTML przed modyfikacjƒÖ
        
        Args:
            html_path: ≈öcie≈ºka do pliku HTML
            
        Returns:
            ≈öcie≈ºka do backupu lub None je≈õli siƒô nie powiod≈Ço
        """
        if not self.backup_enabled or not html_path.exists():
            return None
        
        backup_dir = Path("backups")
        backup_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = html_path.stem
        backup_path = backup_dir / f"{filename}_backup_{timestamp}.html"
        
        try:
            shutil.copy2(html_path, backup_path)
            self.log(f"  üíæ Backup: {backup_path.name}")
            return backup_path
        except Exception as e:
            self.log(f"  ‚ö†Ô∏è  B≈ÇƒÖd backupu: {str(e)}")
            return None

    def scan_directory(self, folder_path: Path, folder_name: str) -> Dict[str, List[Dict]]:
        """
        v4.1: Skanuje folder hierarchicznie z cache'em (60% szybciej)

        Returns:
            Scanned structure (from cache if unchanged)
        """
        # v4.1: Sprawd≈∫ czy folder siƒô zmieni≈Ç
        if not self._has_folder_changed(folder_path, folder_name):
            cached = self.structure_cache.get(folder_name, {})
            self.log(f"  ‚ö° Cache: {folder_name} ({len(cached)} sekcji)")
            return cached

        structure = {}
        
        if not folder_path.exists():
            self.log(f"  ‚ö†Ô∏è  Folder nie istnieje: {folder_path}")
            return structure
        
        for item in sorted(folder_path.iterdir()):
            if item.name.startswith('.'):
                continue
            
            if item.is_file() and item.suffix == '.html':
                self._add_task(structure, "", "", item, folder_name)
            elif item.is_dir():
                self._process_section(structure, item, folder_name)
        
        # v4.1: Cache wynik
        self.structure_cache[folder_name] = structure
        self.log(f"  üì• Skanowano: {folder_name} ({len(structure)} sekcji)")

        return structure

    def _process_section(self, structure: Dict, section_path: Path, folder_name: str):
        """Przetwarza folder sekcji"""
        section_name = section_path.name
        html_files = list(section_path.glob("*.html"))
        subdirs = [d for d in section_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
        
        if html_files and not subdirs:
            for html_file in sorted(html_files):
                self._add_task(structure, section_name, "", html_file, folder_name)
        elif subdirs and not html_files:
            for subdir in sorted(subdirs):
                self._process_subsection(structure, section_name, subdir, folder_name)
        elif html_files and subdirs:
            for html_file in sorted(html_files):
                self._add_task(structure, section_name, "", html_file, folder_name)
            for subdir in sorted(subdirs):
                self._process_subsection(structure, section_name, subdir, folder_name)

    def _process_subsection(self, structure: Dict, section_name: str, subsection_path: Path, folder_name: str):
        """Przetwarza folder podsekcji"""
        subsection_name = subsection_path.name
        index_file = subsection_path / "index.html"
        subfolders = [d for d in subsection_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
        
        if index_file.exists():
            self._add_task(structure, section_name, subsection_name, subsection_path, folder_name)
        elif subfolders:
            for subfolder in sorted(subfolders):
                self._add_task(structure, section_name, subsection_name, subfolder, folder_name)

    def _add_task(self, structure: Dict, section_name: str, subsection_name: str, item_path: Path, folder_name: str):
        """Dodaje zadanie do struktury"""
        if not section_name:
            section_name = "Pozosta≈Çe"
        
        if section_name not in structure:
            structure[section_name] = []
        
        task_info = self._create_task_info(item_path, section_name, subsection_name, folder_name)
        
        if subsection_name:
            found = False
            for item in structure[section_name]:
                if item.get("type") == "subsection" and item.get("name") == subsection_name:
                    item["tasks"].append(task_info)
                    found = True
                    break
            
            if not found:
                structure[section_name].append({
                    "type": "subsection",
                    "name": subsection_name,
                    "tasks": [task_info]
                })
        else:
            structure[section_name].append(task_info)

    def _create_task_info(self, item_path: Path, section_name: str, subsection_name: str, folder_name: str) -> Dict[str, str]:
        """Tworzy info o zadaniu"""
        is_folder = item_path.is_dir()
        
        if is_folder:
            title = subsection_name
            relative_path = item_path.name
        else:
            title = item_path.stem
            relative_path = item_path.name
        
        url_parts = [self.BASE_URL, folder_name]
        
        if section_name:
            url_parts.append(section_name.replace(" ", "%20"))
        if subsection_name and subsection_name != section_name:
            url_parts.append(subsection_name.replace(" ", "%20"))
        
        url_base = "/".join(url_parts)
        
        if is_folder:
            url = f"{url_base}/{relative_path}/index.html"
        else:
            url = f"{url_base}/{relative_path}"
        
        url = url.replace("//", "/").replace(":/", "://")
        
        description = self._extract_description(title)
        
        return {
            "title": title,
            "description": description,
            "url": url,
            "type": "folder" if is_folder else "file"
        }

    def _extract_description(self, name: str) -> str:
        """
        Generuje opisowy tekst z cache'iem i zaawansowanymi regex

        Args:
            name: Nazwa do przetworzenia

        Returns:
            Opisowy tekst
        """
        # Sprawdzenie cache'u
        if name in self.description_cache:
            return self.description_cache[name]

        # Mapowanie specjalnych nazw
        special_mappings = {
            "index": "Strona g≈Ç√≥wna",
            "readme": "README",
            "template": "Szablon",
            "example": "Przyk≈Çad",
            "sample": "Przyk≈Çad",
            "main": "G≈Ç√≥wny plik",
            "start": "Start",
            "begin": "PoczƒÖtek"
        }

        name_lower = name.lower()
        if name_lower in special_mappings:
            result = special_mappings[name_lower]
            self.description_cache[name] = result
            return result

        # Zaawansowane regex patterns
        desc = name.replace("-", " ").replace("_", " ").replace(".", " ")

        # Wzorce numeryczne
        patterns = [
            (r'^zadanie(\d+)', r'Zadanie \1'),
            (r'^zad(\d+)', r'Zadanie \1'),
            (r'^ƒáwiczenie(\d+)', r'ƒÜwiczenie \1'),
            (r'^cw(\d+)', r'ƒÜwiczenie \1'),
            (r'^lab(\d+)', r'Laboratorium \1'),
            (r'^projekt(\d+)', r'Projekt \1'),
            (r'^test(\d+)', r'Test \1'),
            (r'^quiz(\d+)', r'Quiz \1'),
            (r'^lekcja(\d+)', r'Lekcja \1'),
            (r'^rozdzial(\d+)', r'Rozdzia≈Ç \1'),
            (r'^chapter(\d+)', r'Chapter \1'),
        ]

        for pattern, replacement in patterns:
            desc = re.sub(pattern, replacement, desc, flags=re.IGNORECASE)

        # Je≈õli nic nie pasowa≈Ço, u≈ºyj prostego podej≈õcia
        if desc == name.replace("-", " ").replace("_", " ").replace(".", " "):
            desc = desc.strip()

        # Title case - ale zachowaj u≈ºytkownika-friendly format
        result = desc.title() if desc else "Brak opisu"

        # Cache'uj wynik
        self.description_cache[name] = result

        return result

    def _generate_card_html(self, task: Dict[str, str]) -> str:
        """Generuje HTML karty"""
        return f'''            <div class="col-sm-6 col-lg-4">
                <div class="link-card text-white text-decoration-none d-block h-100 position-relative rounded-4 p-4">
                    <a class="stretched-link" href="{task['url']}" target="_blank"></a>
                    <h3 class="mb-3 fs-5 fw-semibold">{task['title']}</h3>
                    <p class="mb-3">{task['description']}</p>
                    <a class="btn btn-sm btn-outline-light d-inline-flex align-items-center gap-2 position-relative" download="" href="{task['url']}" onclick="event.stopPropagation();" style="z-index: 2;">
                        <span>‚¨áÔ∏è</span> Pobierz
                    </a>
                </div>
            </div>
'''

    def _get_existing_urls(self, container) -> Set[str]:
        """Pobiera istniejƒÖce URLe"""
        urls = set()
        for link in container.find_all('a', {'target': '_blank', 'class': 'stretched-link'}):
            if link.get('href'):
                urls.add(link['href'])
        return urls

    def _find_and_remove_card(self, container, url_to_remove: str) -> bool:
        """
        ‚≠ê NOWE: Znajduje i usuwa kartƒô z danym URL'em
        
        Args:
            container: BeautifulSoup container
            url_to_remove: URL karty do usuniƒôcia
            
        Returns:
            True je≈õli usuniƒôto, False je≈õli nie znaleziono
        """
        # Szukaj linku stretched-link z danym URL'em
        for link in container.find_all('a', {'target': '_blank', 'class': 'stretched-link'}):
            if link.get('href') == url_to_remove:
                # Znale≈∫li≈õmy link - szukaj kolumny (col-*)
                parent = link.parent
                while parent:
                    if 'col-sm-6' in parent.get('class', []):
                        parent.decompose()  # Usu≈Ñ element
                        self.log(f"    üóëÔ∏è  Usuniƒôta karta: {url_to_remove}")
                        return True
                    parent = parent.parent
        
        return False

    def remove_obsolete_cards(self, container, valid_urls: Set[str]) -> int:
        """
        ‚≠ê NOWE: Usuwa karty kt√≥re nie istniejƒÖ w validnych URL'ach
        
        Args:
            container: BeautifulSoup container
            valid_urls: Set prawid≈Çowych URL'√≥w
            
        Returns:
            Liczba usuniƒôtych kart
        """
        removed_count = 0
        existing_urls = self._get_existing_urls(container)
        
        for url in existing_urls:
            if url not in valid_urls:
                if self._find_and_remove_card(container, url):
                    removed_count += 1
                    self.removed_urls.add(url)
        
        return removed_count

    def remove_empty_sections(self, container) -> int:
        """
        ‚≠ê NOWE: Usuwa puste sekcje (bez kart)

        Args:
            container: BeautifulSoup container

        Returns:
            Liczba usuniƒôtych sekcji
        """
        removed_count = 0

        # Szukaj wszystkich div.mb-4 (sekcji)
        sections = container.find_all('div', {'class': 'mb-4'})

        for section in sections:
            # Szukaj kart (col-sm-6) w sekcji
            cards = section.find_all('div', class_=lambda x: x and 'col-sm-6' in x)

            # Je≈õli sekcja nie ma ≈ºadnych kart - usu≈Ñ jƒÖ
            if not cards:
                # WyciƒÖgnij nazwƒô sekcji PRZED usuniƒôciem
                h3 = section.find('h3', {'class': 'subsection-title'})
                section_name = h3.get_text(strip=True) if h3 else "Nieznana"

                section.decompose()
                removed_count += 1
                self.log(f"    üóëÔ∏è  Usuniƒôta pusta sekcja: {section_name}")

        return removed_count

    def update_html_file(self, html_path: Path, source_path: Path, folder_name: str) -> bool:
        """
        Aktualizuje plik HTML z obs≈ÇugƒÖ b≈Çƒôd√≥w

        Args:
            html_path: ≈öcie≈ºka do pliku HTML
            source_path: ≈öcie≈ºka do repozytorium ≈∫r√≥d≈Çowego
            folder_name: Nazwa kategorii

        Returns:
            True je≈õli powodzenie, False je≈õli b≈ÇƒÖd
        """
        try:
            source_folder = source_path / folder_name

            if not source_folder.exists():
                self.log(f"  ‚ö†Ô∏è  Folder nie istnieje: {source_folder}")
                return False

            # Skanuj zadania
            structure = self.scan_directory(source_folder, folder_name)

            if not structure:
                self.log(f"  ‚ö†Ô∏è  Brak zada≈Ñ znalezionych w {folder_name}")
                return False

            # Utw√≥rz backup
            backup_path = self.create_backup(html_path)

            try:
                # Otw√≥rz plik HTML
                with open(html_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                soup = BeautifulSoup(content, 'html.parser')
            except UnicodeDecodeError as e:
                self.log(f"  ‚ùå B≈ÇƒÖd kodowania pliku HTML: {str(e)}")
                return False
            except Exception as e:
                self.log(f"  ‚ùå B≈ÇƒÖd odczytu HTML: {str(e)}")
                return False

            # Znajd≈∫ container
            container = soup.find('div', {'class': 'content-wrapper'})
            if not container:
                self.log(f"  ‚ö†Ô∏è  Nie znaleziono content-wrapper w {html_path.name}")
                self.log(f"  üí° Wskaz√≥wka: Struktura HTML mog≈Ça siƒô zmieniƒá")
                return False

            # Pobierz istniejƒÖce URLe
            self.seen_urls = self._get_existing_urls(container)

            # Zbierz wszystkie prawid≈Çowe URL'e
            all_valid_urls: Set[str] = set()

            # Dodaj nowe karty
            added_count = 0
            failed_additions = 0

            for section_name, items in sorted(structure.items()):
                if not items:
                    continue

                for item in items:
                    try:
                        if item.get("type") == "subsection":
                            for task in item.get("tasks", []):
                                all_valid_urls.add(task['url'])
                                if task['url'] not in self.seen_urls:
                                    if self._add_card_to_html(container, soup, section_name, item.get("name"), task):
                                        added_count += 1
                                        self.seen_urls.add(task['url'])
                        else:
                            all_valid_urls.add(item['url'])
                            if item['url'] not in self.seen_urls:
                                if self._add_card_to_html(container, soup, section_name, None, item):
                                    added_count += 1
                                    self.seen_urls.add(item['url'])
                    except Exception as e:
                        failed_additions += 1
                        self.log(f"  ‚ö†Ô∏è  B≈ÇƒÖd dodawania karty: {str(e)}")

            # Usu≈Ñ obsolete karty
            removed_count = self.remove_obsolete_cards(container, all_valid_urls)

            # Usu≈Ñ puste sekcje
            empty_sections_removed = self.remove_empty_sections(container)

            # Zapisz plik z obs≈ÇugƒÖ b≈Çƒôd√≥w
            try:
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup.prettify()))
            except IOError as e:
                self.log(f"  ‚ùå B≈ÇƒÖd zapisu HTML: {str(e)}")
                if backup_path:
                    self.log(f"  üí° Przywracam backup z: {backup_path}")
                    shutil.copy2(backup_path, html_path)
                return False

            # Sprawd≈∫ czy by≈Çy jakiekolwiek zmiany (NOWE v4.0)
            has_changes = added_count > 0 or removed_count > 0 or empty_sections_removed > 0

            self.log(f"  ‚úì +{added_count} -{removed_count} ~{empty_sections_removed} {html_path.name}")
            if failed_additions > 0:
                self.log(f"  ‚ö†Ô∏è  {failed_additions} b≈Çƒôd√≥w przy dodawaniu kart")

            self.changes_summary["modified"].append(html_path.name)
            self.changes_summary["folders_updated"].append(folder_name)

            # Zwr√≥ƒá True tylko je≈õli by≈Çy zmiany (NOWE v4.0)
            return has_changes

        except Exception as e:
            self.log(f"  ‚ùå Nieoczekiwany b≈ÇƒÖd: {str(e)}")
            return False


    def _add_card_to_html(self, container, soup, section_name: str, subsection_name: Optional[str], task: Dict) -> bool:
        """Dodaje kartƒô do istniejƒÖcej sekcji"""
        section_h3 = None
        for h3 in container.find_all('h3', {'class': 'subsection-title'}):
            if h3.get_text(strip=True) == section_name:
                section_h3 = h3
                break
        
        if not section_h3:
            return self._create_and_add_section(container, soup, section_name, subsection_name, task)
        
        if subsection_name:
            h4 = None
            current = section_h3.find_next_sibling()
            while current:
                if current.name == 'h3':
                    break
                if current.name == 'h4' and 'subsection-subtitle' in current.get('class', []):
                    if current.get_text(strip=True) == subsection_name:
                        h4 = current
                        break
                current = current.find_next_sibling()
            
            if h4:
                div_row = h4.find_next_sibling('div', {'class': 'row'})
                if div_row:
                    card_html = BeautifulSoup(self._generate_card_html(task), 'html.parser')
                    div_row.append(card_html)
                    return True
                else:
                    div_row = soup.new_tag('div', attrs={'class': 'row g-3'})
                    card_html = BeautifulSoup(self._generate_card_html(task), 'html.parser')
                    div_row.append(card_html)
                    h4.insert_after(div_row)
                    return True
            else:
                return self._create_subsection_and_add_card(section_h3, soup, subsection_name, task)
        else:
            current = section_h3.find_next_sibling()
            div_row = None
            while current:
                if current.name == 'h3':
                    break
                if current.name == 'div' and 'row' in current.get('class', []):
                    div_row = current
                    break
                current = current.find_next_sibling()
            
            if div_row:
                card_html = BeautifulSoup(self._generate_card_html(task), 'html.parser')
                div_row.append(card_html)
                return True
            else:
                div_row = soup.new_tag('div', attrs={'class': 'row g-3'})
                card_html = BeautifulSoup(self._generate_card_html(task), 'html.parser')
                div_row.append(card_html)
                section_h3.insert_after(div_row)
                return True

    def _create_and_add_section(self, container, soup, section_name: str, subsection_name: Optional[str], task: Dict) -> bool:
        """Tworzy nowƒÖ sekcjƒô"""
        section_div = soup.new_tag('div', attrs={'class': 'mb-4'})
        
        h3 = soup.new_tag('h3', attrs={'class': 'subsection-title fs-4 fw-semibold mb-3'})
        h3.string = section_name
        section_div.append(h3)
        
        if subsection_name:
            h4 = soup.new_tag('h4', attrs={'class': 'subsection-subtitle fs-5 mt-4 mb-3 ms-3'})
            h4.string = subsection_name
            section_div.append(h4)
        
        div_row = soup.new_tag('div', attrs={'class': 'row g-3'})
        card_html = BeautifulSoup(self._generate_card_html(task), 'html.parser')
        div_row.append(card_html)
        section_div.append(div_row)
        
        container.append(section_div)
        return True

    def _create_subsection_and_add_card(self, section_h3, soup, subsection_name: str, task: Dict) -> bool:
        """Tworzy nowƒÖ podsekcjƒô"""
        h4 = soup.new_tag('h4', attrs={'class': 'subsection-subtitle fs-5 mt-4 mb-3 ms-3'})
        h4.string = subsection_name
        section_h3.insert_after(h4)
        
        div_row = soup.new_tag('div', attrs={'class': 'row g-3'})
        card_html = BeautifulSoup(self._generate_card_html(task), 'html.parser')
        div_row.append(card_html)
        h4.insert_after(div_row)
        
        return True

    def run_full_update(self, source_path: Path, target_path: Path) -> bool:
        """
        Pe≈Çna aktualizacja HTML
        
        Returns:
            True je≈õli powodzenie, False je≈õli b≈ÇƒÖd
        """
        self.log("üîÑ Rozpoczynanie aktualizacji...")
        
        # Walidacja repozytori√≥w
        self.log("\nüîç Walidowanie repozytori√≥w:")
        if not self.validate_git_repo(source_path) or not self.validate_git_repo(target_path):
            self.log("‚ùå B≈ÇƒÖd: Repozytoria nie sƒÖ dostƒôpne!")
            return False
        
        # Pull repozytori√≥w
        self.log("\nüì§ Aktualizowanie repozytori√≥w:")
        self.pull_repo(source_path)
        self.pull_repo(target_path)
        
        # Aktualizacja plik√≥w HTML
        self.log("\nüìù Aktualizowanie plik√≥w HTML:")
        changes_found = False  # Tracker zmian (NOWE v4.0)

        for folder in sorted(self.ALLOWED_FOLDERS):
            html_file = target_path / f"{folder}.html"
            if html_file.exists():
                if self.update_html_file(html_file, source_path, folder):
                    changes_found = True  # Oznacz ≈ºe by≈Çy zmiany
            else:
                self.log(f"  ‚ö†Ô∏è  Plik nie istnieje: {html_file.name}")
        
        # Sprawd≈∫ czy by≈Çy jakiekolwiek zmiany (NOWE v4.0)
        if not changes_found:
            self.log("\n" + "=" * 70)
            self.log("‚ÑπÔ∏è  STRONA JEST AKTUALNA - Nie znaleziono ≈ºadnych zmian!")
            self.log("=" * 70)
            return True

        # Commit i push (tylko je≈õli by≈Çy zmiany)
        self.log("\nüì§ Commitowanie i push:")
        self.commit_and_push(target_path)
        
        self.log("\n‚úÖ Aktualizacja zako≈Ñczona!")
        return True

    def commit_and_push(self, repo_path: Path) -> bool:
        """Commitowanie i push zmian"""
        # v4.1: Async commit w osobnym wƒÖtku
        thread = threading.Thread(target=self._commit_and_push_async, args=(repo_path,), daemon=True)
        thread.start()
        return True

    def _commit_and_push_async(self, repo_path: Path):
        """v4.1: Asynchroniczny commit i push"""
        with self.git_lock:
            subprocess.run(
                ["git", "-C", str(repo_path), "add", "-A"],
                capture_output=True
            )

            result = subprocess.run(
                ["git", "-C", str(repo_path), "status", "--porcelain"],
                capture_output=True,
                text=True
            )

            if result.stdout.strip():
                commit_msg = self._generate_commit_message()

                result = subprocess.run(
                    ["git", "-C", str(repo_path), "commit", "-m", commit_msg],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    self.log(f"  ‚úì Commit: {commit_msg.split(chr(10))[0]}")
                else:
                    self.log(f"  ‚ö†Ô∏è  Commit failed: {result.stderr}")
                    return

                result = subprocess.run(
                    ["git", "-C", str(repo_path), "push"],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    self.log(f"  ‚úì Push uko≈Ñczony")
                else:
                    self.log(f"  ‚ö†Ô∏è  Push failed: {result.stderr}")
            else:
                self.log(f"  ‚ÑπÔ∏è  Brak zmian do commitowania")

    # v4.1: DIFF & COMPARISON
    def _get_html_diff(self, old_content: str, new_content: str) -> Dict[str, Any]:
        """
        v4.1: Por√≥wnaj stary i nowy HTML - zwr√≥ƒá statystykƒô zmian

        Returns:
            Dict z informacjami o zmianach
        """
        try:
            old_soup = BeautifulSoup(old_content, 'html.parser')
            new_soup = BeautifulSoup(new_content, 'html.parser')

            # Sprawd≈∫ liczƒô kart
            old_cards = len(old_soup.find_all('div', class_=lambda x: x and 'col-sm-6' in x))
            new_cards = len(new_soup.find_all('div', class_=lambda x: x and 'col-sm-6' in x))

            # Sprawd≈∫ sekcji
            old_sections = len(old_soup.find_all('h3', {'class': 'subsection-title'}))
            new_sections = len(new_soup.find_all('h3', {'class': 'subsection-title'}))

            return {
                'cards_added': max(0, new_cards - old_cards),
                'cards_removed': max(0, old_cards - new_cards),
                'sections_added': max(0, new_sections - old_sections),
                'sections_removed': max(0, old_sections - new_sections),
                'cards_total': new_cards,
                'sections_total': new_sections
            }
        except Exception:
            return {'error': True}

    def _generate_diff_report(self, diff_stats: Dict[str, Any], file_name: str) -> str:
        """v4.1: Generuj raport zmian"""
        if diff_stats.get('error'):
            return f"    ‚ö†Ô∏è Nie mo≈ºna por√≥wnaƒá {file_name}"

        report = f"    üìä {file_name}:"
        if diff_stats['cards_added'] > 0:
            report += f" +{diff_stats['cards_added']}üÉè"
        if diff_stats['cards_removed'] > 0:
            report += f" -{diff_stats['cards_removed']}üóëÔ∏è"
        if diff_stats['sections_added'] > 0:
            report += f" +{diff_stats['sections_added']}üìÅ"

        return report

    # v4.1: BATCH PROCESSING
    def _process_html_file_batch(self, args: Tuple[Path, Path, str]) -> Tuple[str, bool]:
        """
        v4.1: Przetw√≥rz plik HTML w batch processing

        Args:
            args: Tuple (html_path, source_path, folder_name)

        Returns:
            Tuple (folder_name, has_changes)
        """
        html_path, source_path, folder_name = args

        try:
            has_changes = self.update_html_file(html_path, source_path, folder_name)
            return (folder_name, has_changes)
        except Exception as e:
            self.log(f"  ‚ùå B≈ÇƒÖd batch: {folder_name}: {str(e)}")
            return (folder_name, False)

    def run_full_update_batch(self, source_path: Path, target_path: Path) -> bool:
        """
        v4.1: Pe≈Çna aktualizacja HTML z batch processing (3x szybciej!)

        Returns:
            True je≈õli powodzenie, False je≈õli b≈ÇƒÖd
        """
        self.log("üîÑ Rozpoczynanie aktualizacji (v4.1 - batch)...")

        # Walidacja repozytori√≥w
        self.log("\nüîç Walidowanie repozytori√≥w:")
        if not self.validate_git_repo(source_path) or not self.validate_git_repo(target_path):
            self.log("‚ùå B≈ÇƒÖd: Repozytoria nie sƒÖ dostƒôpne!")
            return False

        # v4.1: Pull repozytori√≥w asynchronicznie
        self.log("\nüì§ Aktualizowanie repozytori√≥w:")
        threads = []
        for path in [source_path, target_path]:
            t = threading.Thread(target=self.pull_repo, args=(path,), daemon=True)
            threads.append(t)
            t.start()

        for t in threads:
            t.join()

        # v4.1: Batch processing plik√≥w HTML
        self.log("\nüìù Aktualizowanie plik√≥w HTML (batch mode):")

        html_files = [
            (target_path / f"{folder}.html", source_path, folder)
            for folder in sorted(self.ALLOWED_FOLDERS)
            if (target_path / f"{folder}.html").exists()
        ]

        if not html_files:
            self.log("  ‚ö†Ô∏è  Brak plik√≥w HTML do przetworzenia")
            return False

        changes_found = False

        # v4.1: ThreadPoolExecutor dla batch processing
        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            futures = {
                executor.submit(self._process_html_file_batch, args): args[2]
                for args in html_files
            }

            for future in as_completed(futures):
                try:
                    folder_name, has_changes = future.result()
                    if has_changes:
                        changes_found = True
                except Exception as e:
                    self.log(f"  ‚ùå B≈ÇƒÖd: {str(e)}")

        # Sprawdzenie zmian
        if not changes_found:
            self.log("\n" + "=" * 70)
            self.log("‚ÑπÔ∏è  STRONA JEST AKTUALNA - Nie znaleziono ≈ºadnych zmian!")
            self.log("=" * 70)
            return True

        # v4.1: Asynchroniczny commit i push
        self.log("\nüì§ Commitowanie i push (async):")
        self.commit_and_push(target_path)

        # v4.1: Zapisz cache
        self._save_structure_cache()

        self.log("\n‚úÖ Aktualizacja zako≈Ñczona (v4.1)!")
        return True

    def _generate_commit_message(self) -> str:
        """Generuje wiadomo≈õƒá commita"""
        summary = self.changes_summary
        lines = ["üîÑ Aktualizacja zawarto≈õci stron z szko≈Ça25-26"]
        lines.append("")
        
        if summary["modified"]:
            lines.append("üìù Zaktualizowane strony:")
            for item in set(summary["modified"]):
                lines.append(f"  - {item}")
        
        if summary["folders_updated"]:
            lines.append(f"\nüìÅ Foldery: {', '.join(sorted(set(summary['folders_updated'])))}")
        
        if self.removed_urls:
            lines.append(f"\nüóëÔ∏è  Usuniƒôte: {len(self.removed_urls)} kart")
        
        return "\n".join(lines)

    def validate_html(self, html_path: Path) -> Tuple[bool, List[str]]:
        """
        ‚≠ê NOWE: Waliduje czy HTML jest poprawny

        Args:
            html_path: ≈öcie≈ºka do pliku HTML

        Returns:
            Tuple (is_valid, errors) - czy HTML jest poprawny i lista b≈Çƒôd√≥w
        """
        errors = []

        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()

            soup = BeautifulSoup(content, 'html.parser')

            # Sprawdzenie 1: Czy istnieje content-wrapper
            if not soup.find('div', {'class': 'content-wrapper'}):
                errors.append("Brak div.content-wrapper")

            # Sprawdzenie 2: Czy wszystkie col-* sƒÖ w row
            cols = soup.find_all('div', class_=lambda x: x and 'col-' in x)
            for col in cols:
                parent = col.parent
                grandparent = parent.parent if parent else None
                has_row_parent = (
                    parent and 'row' in parent.get('class', []) or
                    grandparent and 'row' in grandparent.get('class', [])
                )
                if not has_row_parent:
                    errors.append(f"Element col-* nie jest w row: {col.get('class')}")
                    break

            # Sprawdzenie 3: Czy nie ma podw√≥jnych slash√≥w w URL
            links = soup.find_all('a', href=True)
            for link in links:
                url = link.get('href', '')
                if 'prakt.dziadu.dev' in url and '//' in url.split('prakt.dziadu.dev')[1]:
                    errors.append(f"Podw√≥jny slash w URL: {url}")

            # Sprawdzenie 4: Czy spacje sƒÖ enkodowane w URL
            for link in links:
                url = link.get('href', '')
                if ' ' in url:
                    errors.append(f"Nieenkodowana spacja w URL: {url}")

            # Sprawdzenie 5: Czy wszystkie tagi HTML sƒÖ zamkniƒôte
            if content.count('<') != content.count('>'):
                errors.append("Niezamkniƒôte tagi HTML")

            # Sprawdzenie 6: Czy nie ma duplikat√≥w kart
            stretched_links = soup.find_all('a', {'target': '_blank', 'class': 'stretched-link'})
            urls = [link.get('href') for link in stretched_links]
            if len(urls) != len(set(urls)):
                duplicates = [url for url in urls if urls.count(url) > 1]
                errors.append(f"Zduplikowane karty: {len(set(duplicates))} unikalnych duplikat√≥w")

            is_valid = len(errors) == 0
            if is_valid:
                self.log(f"  ‚úÖ HTML poprawny: {html_path.name}")
            else:
                self.log(f"  ‚ö†Ô∏è B≈Çƒôdy w HTML ({len(errors)}): {html_path.name}")
                for error in errors:
                    self.log(f"     - {error}")

            return is_valid, errors

        except Exception as e:
            error_msg = f"B≈ÇƒÖd walidacji HTML: {str(e)}"
            errors.append(error_msg)
            self.log(f"  ‚ùå {error_msg}")
            return False, errors

    def cleanup_old_backups(self, days: int = 30) -> int:
        """
        ‚≠ê NOWE: Usuwa stare backupy starsze ni≈º N dni

        Args:
            days: Liczba dni - backupy starsze bƒôdƒÖ usuniƒôte

        Returns:
            Liczba usuniƒôtych backup√≥w
        """
        from datetime import timedelta
        backup_dir = Path("backups")
        if not backup_dir.exists():
            return 0

        removed_count = 0
        now = datetime.now()
        cutoff_time = now - timedelta(days=days)

        try:
            for backup_file in backup_dir.glob("*.backup"):
                # Sprawd≈∫ czas modyfikacji pliku
                file_mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)

                if file_mtime < cutoff_time:
                    backup_file.unlink()
                    self.log(f"  üóëÔ∏è  Usuniƒôty stary backup: {backup_file.name}")
                    removed_count += 1

            if removed_count > 0:
                self.log(f"  ‚úì Usuniƒôto {removed_count} starych backup√≥w (>{days} dni)")

            return removed_count

        except Exception as e:
            self.log(f"  ‚ö†Ô∏è  B≈ÇƒÖd przy czyszczeniu backup√≥w: {str(e)}")
            return 0

    def get_detailed_log(self) -> str:
        """‚≠ê NOWE: Zwraca szczeg√≥≈Çowy log zmian"""
        return "\n".join(self.detailed_log)

    def scan_media_files(self, source_path: Path, folder_name: str) -> Dict[str, List[str]]:
        """
        ‚≠ê NOWE: Skanuje media pliki w folderach

        Args:
            source_path: ≈öcie≈ºka do repozytorium
            folder_name: Nazwa kategorii

        Returns:
            Dict z mediaami ppogrupowanymi po typach
        """
        media_extensions = {
            'images': ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp'],
            'videos': ['.mp4', '.avi', '.mov', '.mkv', '.webm'],
            'documents': ['.pdf', '.doc', '.docx', '.xls', '.xlsx'],
            'archives': ['.zip', '.rar', '.7z', '.tar', '.gz']
        }

        media_files = {media_type: [] for media_type in media_extensions}

        try:
            folder_path = source_path / folder_name

            for ext_type, extensions in media_extensions.items():
                for ext in extensions:
                    files = list(folder_path.rglob(f'*{ext}'))
                    files += list(folder_path.rglob(f'*{ext.upper()}'))
                    media_files[ext_type].extend([f.name for f in files])

            # Loguj wyniki
            for media_type, files in media_files.items():
                if files:
                    self.log(f"  üìé {media_type.capitalize()}: {len(set(files))} plik√≥w")

            return media_files

        except Exception as e:
            self.log(f"  ‚ö†Ô∏è  B≈ÇƒÖd skanowania medi√≥w: {str(e)}")
            return media_files
